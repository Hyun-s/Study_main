{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d744a15b",
   "metadata": {},
   "source": [
    "## GBDT(gradient boosting decision tree)  \n",
    "- 수치의 크기(범위)가 아닌 대소관계에만 영향을 받는다.  \n",
    "- 결측값이 있어도 그대로 처리할 수 있다.  \n",
    "- 결정트리의 내부 반복작업에 따라 변수 간 상호작용을 반영한다.  \n",
    "- one-hot encoding 이 아닌 label encoding 사용 가능하다.(신경망은 one-hot encoding)  \n",
    "    DT의 경우는 각 데이터의 합이 아닌 대소 관계를 보기 때문? and 신경망은 데이터의 선형 결합(합)으로 이루어져 있기 때문?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e2a4f",
   "metadata": {},
   "source": [
    "### 결측값 처리하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff20efd3",
   "metadata": {},
   "source": [
    "위의 GBDT는 결측값을 하나의 feature로 사용 가능하다.  \n",
    "이 결측값을 처리하는 방법은 아래와 같다.  \n",
    "- 결측값을 해당 변수의 대푯값(중앙값, 평균값, 로그로 변환후 평균, 그룹화 후 그룹별 평균 등)으로 설정  \n",
    "- 베이즈 평균  \n",
    "###    $\\bar{x} = {\\sum_{i=1}^{n}{x_i} + Cm \\over n + C}$    \n",
    "  \n",
    "    $\\sum_{i=1}^{n}{x_i}$ =  기존 평균 계산을 위한 기존 데이터의 총 합  \n",
    "    $Cm$ = m이라는 값을 가진 데이터가 C개 있다는 가정, 이때의 총 합  \n",
    "    $n + C$ = 기존 데이터 n 개 가정 데이터 C개로 총 데이터 개수  \n",
    "    $\\bar x$ = 기존 데이터 n개와 m 값을 가진 데이터가 C개 있을 때의 평균  \n",
    "  \n",
    "  \n",
    "### 다른 변수로부터 결측값 예측하기  \n",
    "추가적인 모델 구축, 결측값을 test 데이터로, 결측값이 아닌 것을 train 데이터로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd2ac44",
   "metadata": {},
   "source": [
    "## 수치형 변수 변환  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14460f0c",
   "metadata": {},
   "source": [
    "### 선형 변환  \n",
    "이는 단순 사칙연산을 사용하여 변수 변환을 하므로 분포의 형태는 변하지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb634c91",
   "metadata": {},
   "source": [
    "#### 1. 표준화  \n",
    "$x' = {x - \\mu \\over \\sigma}$ 로 표준화  \n",
    "1. train의 평균, 표준편차만 사용하여 test를 변환  \n",
    "2. train, test 결합 후의 평균, 표준편차 사용하여 test 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c82bfd",
   "metadata": {},
   "source": [
    "#### 2. 최소-최대 스케일링  \n",
    "$x' = {x - x_{min} \\over x_{max} - x_{min}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ada953",
   "metadata": {},
   "source": [
    "### 비선형 변환  \n",
    "이는 데이터의 분포 형태를 바꾼다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe91ec4",
   "metadata": {},
   "source": [
    "#### 박스-칵스 변환, 여-존스 변환  \n",
    "box-cox transform  \n",
    "$x^{\\lambda} = {x^{\\lambda} - 1 \\over \\lambda} ~~~~~~~~~~~~~~~~~~~~~~~~ if~~ \\lambda !=0$  \n",
    "$~~~~ = logx  ~~~~~~~~~~~~~~~~~~~~~~~~ if~~ \\lambda==0$  \n",
    "  \n",
    "Yeo-Johnson transform(x가 음수도 가질 때)  \n",
    "$x^{\\lambda} = {x^{\\lambda} - 1 \\over \\lambda} ~~~~~~~~~~~~~~~~~~~~~~~~ if~~ \\lambda !=0,x_i\\ge 0$  \n",
    "$~~~~ = logx  ~~~~~~~~~~~~~~~~~~~~~~~~ if~~ \\lambda==0,x_i\\ge 0$  \n",
    "$~~~~ = {-[(-x+1)^{2-\\lambda}-1] \\over 2-\\lambda}  ~~~~~~~~~~ if~~ \\lambda!=2,x_i\\le 0$  \n",
    "$~~~~ = -log(-x+1) ~~~~~~~~~ if~~ \\lambda==2,x_i\\le 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0af063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler, PowerTransformer\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "pt = PowerTransformer(method='box-cox')\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "scaler.fit(train_x[num_cols])\n",
    "\n",
    "train[num_cols] = scaler.transform(train_x[num_cols])\n",
    "test[num_cols] = scaler.transform(train_x[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166e7a5",
   "metadata": {},
   "source": [
    "### 클리핑  \n",
    "상한, 하한을 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abbb421",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-17T09:46:40.628992Z",
     "start_time": "2021-08-17T09:46:40.360885Z"
    }
   },
   "outputs": [],
   "source": [
    "p01 = train_x[num_cols].quantile(0.01)\n",
    "p99 = train_x[num_cols].quantile(0.99)\n",
    "\n",
    "train_x[num_cols] = train_x[num_cols].clip(p01,p99,axis=1)\n",
    "test_x[num_cols] = train_x[num_cols].clip(p01,p99,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef40c9",
   "metadata": {},
   "source": [
    "### 구간 분할  \n",
    "구간 분할시 수치형 데이터는 범주형 데이터로 변환되어 one-hot encoding 가능해짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88eb0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,7,5,4,6,3]\n",
    "\n",
    "binned = pd.cut(x,3,labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898220e2",
   "metadata": {},
   "source": [
    "### 순위로 변환  \n",
    "수치형 변수를 대소 관계에 따른 순위로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0d3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [1,7,5,4,6,3]\n",
    "\n",
    "rank = pd.Series(x).rank()\n",
    "rank = np.argsort(np.argsort(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74e14ac",
   "metadata": {},
   "source": [
    "### RankGauss  \n",
    "수치형 변수를 순위로 변환한 뒤 순서를 유지한 채 정규분포로 변환  \n",
    "신경망에서 일반적인 표준화보다 좋은 성능을 냄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425459b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# n_quantiles 순위 개수, output_distribution을 정규분포로\n",
    "transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\n",
    "\n",
    "transformer.fit(train_x[num_cols])\n",
    "\n",
    "train_x[num_cols] = transformer.transform(train_x[num_cols])\n",
    "test_x[num_cols] = transformer.transform(test_x[num_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313a38e8",
   "metadata": {},
   "source": [
    "## 범주형 변수 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68af1173",
   "metadata": {},
   "source": [
    "### one-hot encoding  \n",
    "범주형 변수의 레벨이 n개일 때 가변수를 해당 레벨 개수만큼 만들면 다중공선성이 생김.  \n",
    "  \n",
    "- pandas 에서는 get_dummies로 사용 가능  \n",
    "- sklearn 에서는 sklearn.preprocessing.OneHotEncoder 에서 사용 가능  \n",
    "    이 경우 sparse=True 사용하여 희소행렬 리턴, 메모리 절약가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4811e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "all_x pd.concat([train_x,test_x])\n",
    "all_x = pd.get_dummies(all_x, columns=cat_cols)\n",
    "\n",
    "train_x = all_x.iloc[:train_x.shape[0],:].reset_index(drop=True)\n",
    "test_x = all_x.iloc[train_x.shaple[0:],:].reset_index(drop=True)\n",
    "\n",
    "# sklearn\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse=False, categories='auto')\n",
    "ohe.fit(train_x[cat_cols])\n",
    "\n",
    "columns = [] # 컬럼 명 생성\n",
    "for i, c in enumerate(cat_cols):\n",
    "    columns += [f'{c}_{v}' for v in ohe.categories_[i]]\n",
    "    \n",
    "dummy_vals_train = pd.DataFrame(ohe.transform(train_x[cat_cols]), columns=columns)\n",
    "dummy_vals_test = pd.DataFrame(ohe.transform(test_x[cat_cols]), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d15dc",
   "metadata": {},
   "source": [
    "### label encoding  \n",
    "one-hot encoding과는 다르게 대소 구분이 형성됨  \n",
    "일반적인 tree 베이스 모델에 자주 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396052b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocesing import LabelEncoder\n",
    "\n",
    "for c in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_x[c])\n",
    "    train_x[c] = le.transform(train_x[c])\n",
    "    test_x[c] = le.transform(test_x[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eebbe6d",
   "metadata": {},
   "source": [
    "### 특징 해싱(feature hashing)  \n",
    "범주형 변수의 레벨 수가 많고, one-hot encoding시 생성되는 feature가많을 때, 해시함수를 이용  \n",
    "이 경우 다른 feature간의 공통된 영역에 플래그가 표시가능함.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b6c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "for  c in cat_cols:\n",
    "    fh = FeatureHasher(n_features=5, input_type='string')\n",
    "    \n",
    "    hash_train = fh.transform(train_x[[c]].astype(str).values)\n",
    "    hash_test = fh.transform(test_x[[c]].astype(str).values)\n",
    "    \n",
    "    hash_train = pd.DataFrame(hash_train.todense(), columns=[f'{c}_{i}' for i in range(5)])\n",
    "    hash_test = pd.DataFrame(hash_test.todense(), columns=[f'{c}_{i}' for i in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59614b00",
   "metadata": {},
   "source": [
    "### 프리퀀시 인코딩(frequency encoding)  \n",
    "각 레벨의 출현 횟수, 출현 빈도로 변수를 대체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd8f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in cat_cols:\n",
    "    freq = train_x[c].value_counts()\n",
    "    \n",
    "    train_x[c] = train_x[c].map(freq)\n",
    "    test_x[c] = test_x[c].map(freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70e711",
   "metadata": {},
   "source": [
    "### 타겟 인코딩(target encoding)  \n",
    "목적 변수를 이용하여 범주형 변수를 수치형 변수로 변환  \n",
    "target이란 a1이라는 feature에서 목적변수의 평균 등 있음.  \n",
    "목적 변수의 데이터 정보를 누출할 우려 있음. 시계열에선 좋지 못함.  \n",
    "  \n",
    "전체 데이터에서 계산 시 정보 누출 가능성 있음. 그러니 k-fold 사용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a9ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# for문을 이용한 변수를 반복하여 타깃 인코딩 수행\n",
    "for c in cat_cols:\n",
    "    # 학습 데이터 전체에서 각 범주별 타깃 평균을 계산\n",
    "    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n",
    "    target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "\n",
    "    # 테스트 데이터의 카테고리 변경\n",
    "    test_x[c] = test_x[c].map(target_mean)\n",
    "\n",
    "    # 학습 데이터 변환 후 값을 저장하는 배열을 준비\n",
    "    tmp = np.repeat(np.nan, train_x.shape[0])\n",
    "\n",
    "    # 학습 데이터 분할\n",
    "    kf = KFold(n_splits=4, shuffle=True, random_state=72)\n",
    "    for idx_1, idx_2 in kf.split(train_x):\n",
    "        # 아웃 오브 폴드로 각 범주형 목적변수 평균 계산\n",
    "        target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "        # 변환 후의 값을 날짜 배열에 저장\n",
    "        tmp[idx_2] = train_x[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "    # 변환 후의 데이터로 원래의 변수를 변경\n",
    "    train_x[c] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ccf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 교차 검증 폴드마다 타깃 인코딩 다시 적용\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "\n",
    "    # 학습 데이터에서 학습 데이터와 검증 데이터 구분\n",
    "    tr_x, va_x = train_x.iloc[tr_idx].copy(), train_x.iloc[va_idx].copy()\n",
    "    tr_y, va_y = train_y.iloc[tr_idx], train_y.iloc[va_idx]\n",
    "\n",
    "    # 변수를 반복하여 타깃 인코딩 수행\n",
    "    for c in cat_cols:\n",
    "        # 학습 데이터 전체에서 각 범주별 타깃 평균을 계산\n",
    "        data_tmp = pd.DataFrame({c: tr_x[c], 'target': tr_y})\n",
    "        target_mean = data_tmp.groupby(c)['target'].mean()\n",
    "        # 검증 데이터의 카테고리 치환\n",
    "        va_x.loc[:, c] = va_x[c].map(target_mean)\n",
    "\n",
    "        # 학습 데이터 변환 후 값을 저장하는 배열 준비\n",
    "        tmp = np.repeat(np.nan, tr_x.shape[0])\n",
    "        kf_encoding = KFold(n_splits=4, shuffle=True, random_state=72)\n",
    "        for idx_1, idx_2 in kf_encoding.split(tr_x):\n",
    "            # 아웃 오브 폴드에서 각 범주별 목적변수 평균 계산\n",
    "            target_mean = data_tmp.iloc[idx_1].groupby(c)['target'].mean()\n",
    "            # 변환 후 값을 날짜 배열에 저장\n",
    "            tmp[idx_2] = tr_x[c].iloc[idx_2].map(target_mean)\n",
    "\n",
    "        tr_x.loc[:, c] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f70816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 교차 검증의 폴드를 정의\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=71)\n",
    "\n",
    "# 변수를 루프하여 타깃 인코딩 수행\n",
    "for c in cat_cols:\n",
    "\n",
    "    # 타깃을 추가\n",
    "    data_tmp = pd.DataFrame({c: train_x[c], 'target': train_y})\n",
    "    # 변환 후 값을 저장하는 배열을 준비\n",
    "    tmp = np.repeat(np.nan, train_x.shape[0])\n",
    "\n",
    "    # 학습 데이터에서 검증 데이터를 나누기\n",
    "    for i, (tr_idx, va_idx) in enumerate(kf.split(train_x)):\n",
    "        # 학습 데이터에 대해 각 범주별 목적변수 평균 계산\n",
    "        target_mean = data_tmp.iloc[tr_idx].groupby(c)['target'].mean()\n",
    "        # 검증 데이터에 대해 변환 후 값을 날짜 배열에 저장\n",
    "        tmp[va_idx] = train_x[c].iloc[va_idx].map(target_mean)\n",
    "\n",
    "    # 변환 후의 데이터로 원래의 변수를 변경\n",
    "    train_x[c] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d57b5b3",
   "metadata": {},
   "source": [
    "### 임베딩  \n",
    "자연어 처리에서 단어나 범주형 변수와 같은 이산적 표현을 실수 벡터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d053db0",
   "metadata": {},
   "source": [
    "### 날짜 및 시간변수변환  \n",
    "테스트에 있는 연도 정보를 학습 데이터의 최신 연도 정보로 바꾸기  \n",
    "기간을 한정하여 활용  \n",
    "  \n",
    "- 연도  \n",
    "    특징에 포함 x 특징에 포함 시킬 시 테스트에는 최신 연도로 치환  \n",
    "- 월  \n",
    "    계절성을 파악 가능.  \n",
    "    원-핫 인코딩으로도 표현(대소관계 제거를 위해)  \n",
    "- 일  \n",
    "    그대로 원-핫 인코딩 시 변수의 수가 많아짐.  \n",
    "    특정 이벤트 있는 날인지만 가지게 하는 것이 좋음.  \n",
    "- 연월  \n",
    "    연 X 12 + 월  \n",
    "- 연월일  \n",
    "    연 X 10000 + 월 X 100 + 일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b91cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
