## 2. 위에서 75, 76 라인에서 출력하는 값의 차이가 왜 발생하는 지 밝혔다면, 개선할 수 있는 방법을 제시하고, 그 방법을 수행하여 차이를 줄였음으로 보여주세요. (3점)

```
Cost for minimal parameters: 18.15548569192936 , with theta0 = 4.1348591549295834  and theta1 = 1.2063883299798792
Cost for other theta: 116.89777777777779
Gradient descent gives after 100 steps:  [4.13492037 1.20675222]
Best theta:  [4.1348591549295834, 1.2063883299798792]
```

먼저 기본 skeleton code에서 제가 직접 구현한 gradient descent 를 활용하여 theta값을 구한 결과는 위와 같습니다.

현재 Gradient descent gives after 100 steps와 Best theta 와 값의 차이가 근소하게 있음을 확인하였으며 그 이유를 확인하기 위해 각 step당 theta0, theta1, cost를 그래프로 시각화하였고, 그 결과는 아래와 같습니다.



![f1](C:\Users\hyunsoo\3_2\Machine_Learning\f1.png)

현재 이 그래프는 learning rate = 0.01, step = 100 으로 학습한 상태입니다.

보시다시피 t0의 변화는 초기에 값이 상승, 그 후에 최적해를 찾아 가면서 진동하고 있는 것을 볼 수 있습니다.

t1, cost 는 최적해에서 계속 진동함을 확인 할 수 있습니다.

이러한 진동(움직임) 은 **현재 데이터 셋이 완전한 선형관계가 아니기 때문**에 항상 cost가 0이 될 수 없고, 그로 인해 항상 gradient가 0이 아닙니다. 그렇기 때문에 최적해(loss==0)를 찾기 위해서 현재 t0,t1이 움직이게 되는데, 이동량이 너무 커서 최적해를 넘어가고, 그로 인해 다시 최적해를 찾기 위해 반대 방향으로 이동하면서 발생하는 것이 반복됨으로써 생기게 됩니다.

위와 같은 이유로 진동이 발생하고,  이러한 진동 때문에 기존 best theta 값과의 차이가 발생함을 알 수 있습니다.



## 3. 위에서 75, 76 라인에서 출력하는 값의 차이가 왜 발생하는 지 밝혔다면, 개선할 수 있는 방법을 제시하고, 그 방법을 수행하여 차이를 줄였음으로 보여주세요. (3점)

2에서 밝힌 바와 같이 저는 최적해를 찾기 위해 수렴하는 과정에서 이동량(step)이 너무 크기 때문에 발생한다는 것에 초점을 두고 어떻게 개선을 해야할지 고민하였습니다. 그렇게 개선하기 위한 방법은 아래와 같습니다.

1. **Learning Rate 감소**
   파라미터를 갱신할 때에 저희는 learning rate * gradient 을 가감하면서 갱신하게 됩니다. 그렇기 때문에 learning rate를 0.01로 줄여보았습니다. 그 결과는 다음과 같습니다.<img src="C:\Users\hyunsoo\AppData\Roaming\Typora\typora-user-images\image-20211011170306976.png" alt="image-20211011170306976" style="zoom:70%;" />
   기존 base line (2. 에서의 학습 곡선)과 가장 큰 차이점으로는 전반적인 진동의 크기가 줄어듬을 확인하였습니다. 이는 learning rate를 줄였기 때문에 이동량(step)이 줄었기 때문입니다. 그렇기 때문에 값의 차이가 날 수 있는 범위가 줄어 베이스라인에서의 차이보다 줄었음을 알 수 있습니다.

2. **Gradient 계산시 모든 데이터가 아닌 몇몇 데이터만 샘플링 하여 계산**
   파라미터 갱신시에 기존 base line에서는 모든 데이터(같은 데이터)에 기울기를 계산하여 최적해를 찾기 때문에 항상 같은 가설 공간에서 같은 지점만 검색하게 됩니다. 그래서 이러한 문제점을 개선해보고자 기울기를 계산할 시에 모든 데이터가 아닌 몇몇 데이터를 샘플링해서 계산하여 수행해 보았습니다.<img src="C:\Users\hyunsoo\AppData\Roaming\Typora\typora-user-images\image-20211011174744285.png" alt="image-20211011174744285" style="zoom:70%;" />

   위와 같은 그래프는 learning rate = 0.1, step = 100, 샘플링 개수 = 5 으로 base line 과 동일한 조건입니다.
   그래프가 이전보다 불규칙적인 것을 보아 현재 가설에서 문제점과 같이 가설 공간에서 항상 같은 지점이 아닌, 다른 지점도 학습시에 탐색함을 볼 수 있습니다.
   하지만, 몇몇 데이터만을 학습시키다 보니 기존과는 다르게 학습이 불안정함을 보입니다. 그렇기 때문에 출력하는 값의 차이를 줄이는 것이 목표인 이번 과제에서 이 방법은 learning rate를 감소시키는 것보다 적절하지 못해 보입니다.

### Conlusion

위의 몇몇 실험을 통해서 내린 결론은 다음과 같습니다.

출력값의 차이는 gradient를 계산하고 다음 step으로 이동하게 되는데, **이때 데이터 셋이 선형 관계가 아니기 때문에 항상 gradient가 0이 될 수 없어서** 최적해가 넘어가고, 그로 인해 다시 최적해를 찾으려 다시 이동하는 것이 반복됨으로써 발생하는 것이라 생각합니다.

값의 차이를 줄이기 위해 했던 실험들 중 가장 최선의 방법은  step의 크기(learning rate)를 줄임으로써 개선할 수 있었습니다.

그래서 학습률만 줄인 최종 결과는 아래와 같습니다.

```
Cost for minimal parameters: 18.15548569192936 , with theta0 = 4.1348591549295834  and theta1 = 1.2063883299798792
Cost for other theta: 116.89777777777779
Gradient descent gives after 100 steps:  [4.13486528 1.20642472]
Best theta:  [4.1348591549295834, 1.2063883299798792]
```

